# Supabase Configuration
# Go to: Supabase Dashboard > Settings > API
SUPABASE_URL=https://cvfhjxiqmupevkvdulzv.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImN2ZmhqeGlxbXVwZXZrdmR1bHp2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3Njk4NzA0MTcsImV4cCI6MjA4NTQ0NjQxN30.HHMMMBaQauU8XhkGe2J7jqtgUxq-VfRWRSrOxJ4A_e8
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImN2ZmhqeGlxbXVwZXZrdmR1bHp2Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTg3MDQxNywiZXhwIjoyMDg1NDQ2NDE3fQ.am5cb7dUeUe_mrPvqx-bTNniDerumG9Eua160OFxXgU

# LLM Provider API Keys (NOT USED in Ollama-only mode)
# Uncomment and add keys when switching back to cloud APIs
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# GOOGLE_API_KEY=your-google-api-key-here
# OPENAI_API_KEY=sk-your-openai-key-here

# Ollama Configuration (LOCAL MODELS - Currently Active)
OLLAMA_BASE_URL=http://localhost:11434

# Council Models (configured in llm_manager.py):
#   PRIMARY_ANALYZER = deepseek-r1:latest  (replaces Claude)
#   SKEPTIC_CRITIC   = mistral:latest      (replaces Gemini)
#   FAILOVER         = llama3.2:3b

# Active models (for reference):
OLLAMA_PRIMARY_MODEL=deepseek-r1:latest
OLLAMA_SKEPTIC_MODEL=mistral:latest
OLLAMA_FAILOVER_MODEL=llama3.2:3b


# Orchestrator Configuration
ORCHESTRATOR_HOST=0.0.0.0
ORCHESTRATOR_PORT=8000

# Failover Configuration
MAX_RETRIES=3
RETRY_DELAY_SECONDS=2
PROVIDER_HEALTH_CHECK_INTERVAL=60
